{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defde75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "import optuna.integration.lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import warnings\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from library.sb_utils import save_file\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a34eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c43bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import TFBertModel\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "305a3c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 256\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "MODEL = \"distilbert\" #\"bert\"\n",
    "MODEL_PATH = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53cf3b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=MAX_LEN):\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    for text in texts:\n",
    "        token = tokenizer(text, max_length=max_len, truncation=True, padding='max_length',\n",
    "                         add_special_tokens=True)\n",
    "        input_ids.append(token['input_ids'])\n",
    "        token_type_ids.append(token['token_type_ids'])\n",
    "        attention_mask.append(token['attention_mask'])\n",
    "    \n",
    "    return np.array(input_ids), np.array(token_type_ids), np.array(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1235bcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.\\\\tokenizer_config.json',\n",
       " '.\\\\special_tokens_map.json',\n",
       " '.\\\\vocab.txt',\n",
       " '.\\\\added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First load the real tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "# Save the loaded tokenizer locally\n",
    "tokenizer.save_pretrained('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12529718",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = tokenizer.sep_token\n",
    "train['inputs'] = train.discourse_type + sep +train.discourse_text\n",
    "new_label = {\"discourse_effectiveness\": {\"Ineffective\": 0, \"Adequate\": 1, \"Effective\": 2}}\n",
    "train = train.replace(new_label)\n",
    "train = train.rename(columns = {\"discourse_effectiveness\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45e81eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>label</th>\n",
       "      <th>inputs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013cc385424</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>Hi, i'm Isaac, i'm going to be writing about h...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>1</td>\n",
       "      <td>Lead[SEP]Hi, i'm Isaac, i'm going to be writin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9704a709b505</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>On my perspective, I think that the face is a ...</td>\n",
       "      <td>Position</td>\n",
       "      <td>1</td>\n",
       "      <td>Position[SEP]On my perspective, I think that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c22adee811b6</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>I think that the face is a natural landform be...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>1</td>\n",
       "      <td>Claim[SEP]I think that the face is a natural l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a10d361e54e4</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>If life was on Mars, we would know by now. The...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>1</td>\n",
       "      <td>Evidence[SEP]If life was on Mars, we would kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>db3e453ec4e2</td>\n",
       "      <td>007ACE74B050</td>\n",
       "      <td>People thought that the face was formed by ali...</td>\n",
       "      <td>Counterclaim</td>\n",
       "      <td>1</td>\n",
       "      <td>Counterclaim[SEP]People thought that the face ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   discourse_id      essay_id  \\\n",
       "0  0013cc385424  007ACE74B050   \n",
       "1  9704a709b505  007ACE74B050   \n",
       "2  c22adee811b6  007ACE74B050   \n",
       "3  a10d361e54e4  007ACE74B050   \n",
       "4  db3e453ec4e2  007ACE74B050   \n",
       "\n",
       "                                      discourse_text discourse_type  label  \\\n",
       "0  Hi, i'm Isaac, i'm going to be writing about h...           Lead      1   \n",
       "1  On my perspective, I think that the face is a ...       Position      1   \n",
       "2  I think that the face is a natural landform be...          Claim      1   \n",
       "3  If life was on Mars, we would know by now. The...       Evidence      1   \n",
       "4  People thought that the face was formed by ali...   Counterclaim      1   \n",
       "\n",
       "                                              inputs  \n",
       "0  Lead[SEP]Hi, i'm Isaac, i'm going to be writin...  \n",
       "1  Position[SEP]On my perspective, I think that t...  \n",
       "2  Claim[SEP]I think that the face is a natural l...  \n",
       "3  Evidence[SEP]If life was on Mars, we would kno...  \n",
       "4  Counterclaim[SEP]People thought that the face ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88ce7cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['inputs'], train['label'], test_size=0.12, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7213aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = bert_encode(X_train.astype(str), tokenizer)\n",
    "X_test = bert_encode(X_test.astype(str), tokenizer)\n",
    "\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b17746a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((X_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((X_test, y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2bfa1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_model, max_len=MAX_LEN):    \n",
    "    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    token_type_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"token_type_ids\")\n",
    "    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "    sequence_output = bert_model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    clf_output = Dropout(.1)(clf_output)\n",
    "    out = Dense(3, activation='softmax')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "793357e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids (InputLayer)    [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]',         \n",
      "                                tentions(last_hidde               'token_type_ids[0][0]']         \n",
      "                                n_state=(None, 256,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model[0][0]']          \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 768)          0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3)            2307        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,484,547\n",
      "Trainable params: 109,484,547\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "CPU times: total: 5.84 s\n",
      "Wall time: 6.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transformer_layer = (TFBertModel.from_pretrained(MODEL_PATH))\n",
    "model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "061842ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/tf_bert_model/bert/encoder/layer_._5/attention/self/Softmax' defined at (most recent call last):\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_3524\\1548209699.py\", line 1, in <cell line: 1>\n      train_history = model.fit(\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1075, in run_call_with_unpacked_inputs\n      Prepare the output of the saved model. Each model must implement this function.\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1103, in call\n      outputs = self.bert(\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1075, in run_call_with_unpacked_inputs\n      Prepare the output of the saved model. Each model must implement this function.\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 863, in call\n      encoder_outputs = self.encoder(\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 548, in call\n      for i, layer_module in enumerate(self.layer):\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 554, in call\n      layer_outputs = layer_module(\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 464, in call\n      self_attention_outputs = self.attention(\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 380, in call\n      self_outputs = self.self_attention(\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 319, in call\n      attention_probs = stable_softmax(logits=attention_scores, axis=-1)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\tf_utils.py\", line 70, in stable_softmax\n      return tf.nn.softmax(logits=logits + 1e-9, axis=axis, name=name)\nNode: 'model/tf_bert_model/bert/encoder/layer_._5/attention/self/Softmax'\nOOM when allocating tensor with shape[16,12,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/tf_bert_model/bert/encoder/layer_._5/attention/self/Softmax}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_27306]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/gpu:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     train_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model/tf_bert_model/bert/encoder/layer_._5/attention/self/Softmax' defined at (most recent call last):\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_3524\\1548209699.py\", line 1, in <cell line: 1>\n      train_history = model.fit(\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1075, in run_call_with_unpacked_inputs\n      Prepare the output of the saved model. Each model must implement this function.\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1103, in call\n      outputs = self.bert(\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1075, in run_call_with_unpacked_inputs\n      Prepare the output of the saved model. Each model must implement this function.\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 863, in call\n      encoder_outputs = self.encoder(\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 548, in call\n      for i, layer_module in enumerate(self.layer):\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 554, in call\n      layer_outputs = layer_module(\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 464, in call\n      self_attention_outputs = self.attention(\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 380, in call\n      self_outputs = self.self_attention(\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 319, in call\n      attention_probs = stable_softmax(logits=attention_scores, axis=-1)\n    File \"C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\transformers\\tf_utils.py\", line 70, in stable_softmax\n      return tf.nn.softmax(logits=logits + 1e-9, axis=axis, name=name)\nNode: 'model/tf_bert_model/bert/encoder/layer_._5/attention/self/Softmax'\nOOM when allocating tensor with shape[16,12,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model/tf_bert_model/bert/encoder/layer_._5/attention/self/Softmax}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_27306]"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=200,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4887cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "200/200 [==============================] - 3579s 18s/step - loss: 0.8276 - accuracy: 0.6391 - val_loss: 0.7882 - val_accuracy: 0.6541\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 3358s 17s/step - loss: 0.8143 - accuracy: 0.6237 - val_loss: 0.7703 - val_accuracy: 0.6578\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 4072s 20s/step - loss: 0.7513 - accuracy: 0.6700 - val_loss: 0.7501 - val_accuracy: 0.6609\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 3309s 17s/step - loss: 0.7623 - accuracy: 0.6544 - val_loss: 0.7438 - val_accuracy: 0.6682\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 3216s 16s/step - loss: 0.7456 - accuracy: 0.6684 - val_loss: 0.7279 - val_accuracy: 0.6666\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=200,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0ba53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
